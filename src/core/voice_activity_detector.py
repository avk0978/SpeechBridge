#!/usr/bin/env python3
"""
Voice Activity Detection (VAD) - –¥–µ—Ç–µ–∫—Ü–∏—è —Ä–µ—á–∏ –≤ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ö
–ü–æ–º–æ–≥–∞–µ—Ç —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã –±–µ–∑ —Ä–µ—á–∏ (—Ç–∏—à–∏–Ω–∞, –º—É–∑—ã–∫–∞, —Ñ–æ–Ω–æ–≤—ã–µ –∑–≤—É–∫–∏)
"""

import logging
import numpy as np
import librosa
from typing import Dict, List, Optional
from pathlib import Path


class VoiceActivityDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä —Ä–µ—á–µ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ä–µ—á–∏"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ —Ä–µ—á–∏
        self.min_speech_energy = 0.01      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω–µ—Ä–≥–∏—è –¥–ª—è —Å—á–∏—Ç—ã–≤–∞–Ω–∏—è —Å–∏–≥–Ω–∞–ª–∞ —Ä–µ—á—å—é
        self.min_spectral_centroid = 1000  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –¥–ª—è —Ä–µ—á–∏
        self.max_spectral_centroid = 8000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –¥–ª—è —Ä–µ—á–∏
        self.min_speech_duration = 0.5     # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ (—Å–µ–∫—É–Ω–¥—ã)
        self.speech_frequency_ratio = 0.3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è —Ä–µ—á–µ–≤—ã—Ö —á–∞—Å—Ç–æ—Ç
        
    def is_speech_segment(self, audio_path: str, threshold: float = 0.5) -> Dict[str, any]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç —Ä–µ—á—å
        
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–∞–∫ —Ä–µ—á—å (0.0-1.0)
            
        Returns:
            dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ —Ä–µ—à–µ–Ω–∏–µ–º
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            audio, sr = librosa.load(audio_path, sr=None)
            file_duration = len(audio) / sr if sr > 0 else 0
            
            self.logger.debug(f"üîç VAD –∞–Ω–∞–ª–∏–∑ {Path(audio_path).name}: –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å={file_duration:.2f}s, —Å–µ–º–ø–ª–æ–≤={len(audio)}")
            
            if len(audio) == 0:
                return {
                    'is_speech': False,
                    'confidence': 0.0,
                    'reason': 'empty_audio',
                    'metrics': {}
                }
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            metrics = self._extract_speech_metrics(audio, sr)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ—á–µ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            speech_score = self._calculate_speech_score(metrics)
            
            is_speech = speech_score >= threshold
            
            result = {
                'is_speech': is_speech,
                'confidence': speech_score,
                'reason': self._get_decision_reason(metrics, speech_score, threshold),
                'metrics': metrics
            }
            
            self.logger.debug(f"VAD –∞–Ω–∞–ª–∏–∑ {Path(audio_path).name}: speech={is_speech}, confidence={speech_score:.3f}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ VAD –∞–Ω–∞–ª–∏–∑–∞ {audio_path}: {e}")
            return {
                'is_speech': True,  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º —Ä–µ—á—å—é –ø—Ä–∏ –æ—à–∏–±–∫–µ
                'confidence': 0.5,
                'reason': f'error: {str(e)}',
                'metrics': {}
            }
    
    def _extract_speech_metrics(self, audio: np.ndarray, sr: int) -> Dict[str, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏"""
        
        # 1. –≠–Ω–µ—Ä–≥–∏—è —Å–∏–≥–Ω–∞–ª–∞ (RMS)
        rms_energy = np.mean(librosa.feature.rms(y=audio)[0])
        
        # 2. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç "—è—Ä–∫–æ—Å—Ç—å" –∑–≤—É–∫–∞)
        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr)[0])
        
        # 3. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –ø–æ–ª–æ—Å–∞ –ø—Ä–æ–ø—É—Å–∫–∞–Ω–∏—è
        spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0])
        
        # 4. Zero Crossing Rate (—á–∞—Å—Ç–æ—Ç–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –Ω—É–ª—è)
        zcr = np.mean(librosa.feature.zero_crossing_rate(audio)[0])
        
        # 5. MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è —Ä–µ—á–∏)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
        mfcc_var = np.var(mfccs, axis=1)  # –í–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å MFCC
        mfcc_mean = np.mean(mfcc_var)
        
        # 6. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç (—Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –ø–∏–∫–∞–º–∏ –∏ –¥–æ–ª–∏–Ω–∞–º–∏ –≤ —Å–ø–µ–∫—Ç—Ä–µ)
        spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr))
        
        # 7. –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        # –†–µ—á—å –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∏—Ç —ç–Ω–µ—Ä–≥–∏—é –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 85-8000 Hz
        stft = librosa.stft(audio)
        freqs = librosa.fft_frequencies(sr=sr)
        
        # –≠–Ω–µ—Ä–≥–∏—è –≤ —Ä–µ—á–µ–≤–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (85-8000 Hz)
        speech_freq_mask = (freqs >= 85) & (freqs <= 8000)
        speech_energy = np.mean(np.abs(stft[speech_freq_mask, :]))
        total_energy = np.mean(np.abs(stft))
        speech_ratio = speech_energy / (total_energy + 1e-10)
        
        return {
            'rms_energy': float(rms_energy),
            'spectral_centroid': float(spectral_centroid),
            'spectral_bandwidth': float(spectral_bandwidth),
            'zero_crossing_rate': float(zcr),
            'mfcc_variance': float(mfcc_mean),
            'spectral_contrast': float(spectral_contrast),
            'speech_frequency_ratio': float(speech_ratio),
            'duration': len(audio) / sr
        }
    
    def _calculate_speech_score(self, metrics: Dict[str, float]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—Ü–µ–Ω–∫—É —Ä–µ—á–µ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (0.0-1.0)"""
        
        score = 0.0
        weights = {
            'energy': 0.2,
            'spectral': 0.25,
            'frequency': 0.3,
            'mfcc': 0.15,
            'duration': 0.1
        }
        
        # 1. –û—Ü–µ–Ω–∫–∞ —ç–Ω–µ—Ä–≥–∏–∏
        if metrics['rms_energy'] > self.min_speech_energy:
            energy_score = min(1.0, metrics['rms_energy'] / 0.1)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0.1
            score += weights['energy'] * energy_score
        
        # 2. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        centroid = metrics['spectral_centroid']
        if self.min_spectral_centroid <= centroid <= self.max_spectral_centroid:
            # –†–µ—á—å –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç —Ü–µ–Ω—Ç—Ä–æ–∏–¥ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 1000-4000 Hz
            spectral_score = 1.0 - abs(centroid - 2500) / 2500  # –û–ø—Ç–∏–º—É–º –≤ —Ä–∞–π–æ–Ω–µ 2500 Hz
            spectral_score = max(0.0, spectral_score)
            score += weights['spectral'] * spectral_score
        
        # 3. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if metrics['speech_frequency_ratio'] >= self.speech_frequency_ratio:
            freq_score = min(1.0, metrics['speech_frequency_ratio'] / 0.8)
            score += weights['frequency'] * freq_score
        
        # 4. MFCC –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (—Ä–µ—á—å –∏–º–µ–µ—Ç –∏–∑–º–µ–Ω—á–∏–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)
        if metrics['mfcc_variance'] > 0.5:
            mfcc_score = min(1.0, metrics['mfcc_variance'] / 2.0)
            score += weights['mfcc'] * mfcc_score
        
        # 5. –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–µ —Ä–µ—á—å)
        if metrics['duration'] >= self.min_speech_duration:
            duration_score = min(1.0, metrics['duration'] / 2.0)  # –ü–æ–ª–Ω—ã–π –±–∞–ª–ª –ø—Ä–∏ 2+ —Å–µ–∫—É–Ω–¥–∞—Ö
            score += weights['duration'] * duration_score
        
        return min(1.0, score)
    
    def _get_decision_reason(self, metrics: Dict[str, float], score: float, threshold: float) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏—á–∏–Ω—É –ø—Ä–∏–Ω—è—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è"""
        
        if score < threshold:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–∏—á–∏–Ω—ã –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            reasons = []
            
            if metrics['rms_energy'] < self.min_speech_energy:
                reasons.append(f"–Ω–∏–∑–∫–∞—è_—ç–Ω–µ—Ä–≥–∏—è({metrics['rms_energy']:.3f})")
            
            if metrics['spectral_centroid'] < self.min_spectral_centroid:
                reasons.append(f"–Ω–∏–∑–∫–∏–π_—Ü–µ–Ω—Ç—Ä–æ–∏–¥({metrics['spectral_centroid']:.0f}Hz)")
            elif metrics['spectral_centroid'] > self.max_spectral_centroid:
                reasons.append(f"–≤—ã—Å–æ–∫–∏–π_—Ü–µ–Ω—Ç—Ä–æ–∏–¥({metrics['spectral_centroid']:.0f}Hz)")
            
            if metrics['speech_frequency_ratio'] < self.speech_frequency_ratio:
                reasons.append(f"–º–∞–ª–æ_—Ä–µ—á–µ–≤—ã—Ö_—á–∞—Å—Ç–æ—Ç({metrics['speech_frequency_ratio']:.3f})")
            
            if metrics['duration'] < self.min_speech_duration:
                reasons.append(f"–∫–æ—Ä–æ—Ç–∫–∏–π_—Å–µ–≥–º–µ–Ω—Ç({metrics['duration']:.1f}s)")
            
            return "–Ω–µ_—Ä–µ—á—å: " + ", ".join(reasons) if reasons else "–Ω–∏–∑–∫–∏–π_–æ–±—â–∏–π_—Å—á–µ—Ç"
        else:
            return f"—Ä–µ—á—å: —Å—á–µ—Ç={score:.3f}"
    
    def filter_speech_segments(self, segments: List[Dict], min_confidence: float = 0.5) -> List[Dict]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–µ—á—å
        
        Args:
            segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –ø—É—Ç—è–º–∏ –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª–∞–º
            min_confidence: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å—á–∏—Ç—ã–≤–∞–Ω–∏—è —Ä–µ—á—å—é
            
        Returns:
            list: –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ VAD
        """
        
        filtered_segments = []
        
        for i, segment in enumerate(segments):
            audio_path = segment.get('path')
            if not audio_path or not Path(audio_path).exists():
                self.logger.warning(f"–°–µ–≥–º–µ–Ω—Ç {i+1}: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω {audio_path}")
                continue
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ—á–µ–≤—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            vad_result = self.is_speech_segment(audio_path, min_confidence)
            
            # –î–æ–±–∞–≤–ª—è–µ–º VAD –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫ —Å–µ–≥–º–µ–Ω—Ç—É
            segment_with_vad = {
                **segment,
                'vad_is_speech': vad_result['is_speech'],
                'vad_confidence': vad_result['confidence'],
                'vad_reason': vad_result['reason'],
                'vad_metrics': vad_result['metrics']
            }
            
            if vad_result['is_speech']:
                filtered_segments.append(segment_with_vad)
                self.logger.info(f"‚úÖ –°–µ–≥–º–µ–Ω—Ç {i+1}: –†–ï–ß–¨ (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {vad_result['confidence']:.3f})")
            else:
                self.logger.info(f"‚ùå –°–µ–≥–º–µ–Ω—Ç {i+1}: –ù–ï –†–ï–ß–¨ ({vad_result['reason']})")
                # –í—Å–µ —Ä–∞–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º —Å–µ–≥–º–µ–Ω—Ç, –Ω–æ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ no_speech
                segment_with_vad['status'] = 'no_speech_vad'
                filtered_segments.append(segment_with_vad)
        
        speech_count = sum(1 for seg in filtered_segments if seg.get('vad_is_speech', False))
        self.logger.info(f"üéØ VAD —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {speech_count}/{len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–µ—á—å")
        
        return filtered_segments