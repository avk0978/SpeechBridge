#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º (speaker diarization)
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PyAnnote –¥–ª—è –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ä–∞–∑–Ω—ã—Ö –≥–æ–≤–æ—Ä—è—â–∏—Ö
"""

import logging
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import subprocess
import json
import tempfile
import librosa
import numpy as np

class SpeakerDiarization:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Ä–µ—á–∏ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º"""
    
    def __init__(self, config=None):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # –ö–∞—Ä—Ç–∞ –≥–æ–ª–æ—Å–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ø–∏–∫–µ—Ä–æ–≤
        self.voice_mapping = {
            'male': ['ru-male-1', 'ru-male-2', 'ru-male-3'],
            'female': ['ru-female-1', 'ru-female-2', 'ru-female-3']
        }
        self.used_voices = {'male': 0, 'female': 0}
        
    def segment_by_speakers(self, audio_path: str, min_speaker_duration: float = 5.0) -> List[Dict]:
        """
        –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
        
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            min_speaker_duration: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞ —Å–ø–∏–∫–µ—Ä–∞
            
        Returns:
            list: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–ø–∏–∫–µ—Ä–∞—Ö
        """
        try:
            self.logger.info(f"üé≠ –ù–∞—á–∏–Ω–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º: {audio_path}")
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç—É—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ –ø–∞—É–∑–∞–º —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏
            segments = self._segment_by_silence_with_speaker_logic(audio_path, min_speaker_duration)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            segments = self._detect_gender_for_segments(segments)
            
            self.logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º")
            return segments
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º: {e}")
            # Fallback –∫ –æ–±—ã—á–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            return self._fallback_segmentation(audio_path)
    
    def _segment_by_silence_with_speaker_logic(self, audio_path: str, min_duration: float) -> List[Dict]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ –ø–∞—É–∑–∞–º —Å –ª–æ–≥–∏–∫–æ–π —Å–ø–∏–∫–µ—Ä–æ–≤
        """
        from pydub import AudioSegment
        from pydub.silence import split_on_silence, detect_silence
        
        self.logger.debug("üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—É–¥–∏–æ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–∏–∫–µ—Ä–æ–≤...")
        
        audio = AudioSegment.from_file(audio_path)
        total_duration = len(audio) / 1000.0
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ (–ú–ï–ù–ï–ï —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è —ç—Ç–æ–≥–æ –≤–∏–¥–µ–æ)
        if total_duration > 300:  # > 5 –º–∏–Ω—É—Ç
            min_silence_len = 1200   # 1.2 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
            silence_thresh = -35
        elif total_duration > 120:  # > 2 –º–∏–Ω—É—Ç—ã  
            min_silence_len = 1000   # 1.0 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤ - –£–í–ï–õ–ò–ß–ï–ù–û
            silence_thresh = -40     # –ú–µ–Ω–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        else:
            min_silence_len = 800    # 0.8 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤
            silence_thresh = -42
            
        self.logger.debug(f"üéõÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: min_silence={min_silence_len}ms, thresh={silence_thresh}dB")
        
        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –ø–∞—É–∑—ã
        silence_segments = detect_silence(
            audio, 
            min_silence_len=min_silence_len,
            silence_thresh=silence_thresh
        )
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –º–µ–∂–¥—É –ø–∞—É–∑–∞–º–∏
        segments = []
        current_pos = 0
        current_speaker = 0  # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Ç–µ–∫—É—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ (0=A, 1=B)
        
        for i, (silence_start, silence_end) in enumerate(silence_segments):
            # –°–µ–≥–º–µ–Ω—Ç –¥–æ –ø–∞—É–∑—ã
            if silence_start > current_pos:
                segment_duration = (silence_start - current_pos) / 1000.0
                
                if segment_duration >= min_duration:
                    # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–∞ –ø–æ –ø–∞—É–∑–∞–º –∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    silence_duration = (silence_end - silence_start) / 1000.0 if i < len(silence_segments) - 1 else 0
                    
                    if len(segments) == 0:
                        # –ü–µ—Ä–≤—ã–π —Å–µ–≥–º–µ–Ω—Ç - –≤—Å–µ–≥–¥–∞ Speaker_A
                        speaker_label = "Speaker_A"
                        current_speaker = 0
                    elif silence_duration > 3.0:  # –¢–æ–ª—å–∫–æ –û–ß–ï–ù–¨ –¥–ª–∏–Ω–Ω–∞—è –ø–∞—É–∑–∞ - —Å–º–µ–Ω–∞ —Å–ø–∏–∫–µ—Ä–∞ (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 2.0)
                        current_speaker = (current_speaker + 1) % 2  # –ß–µ—Ä–µ–¥—É–µ–º –º–µ–∂–¥—É 0 –∏ 1
                        speaker_label = f"Speaker_{chr(65 + current_speaker)}"
                    elif segment_duration > 60:  # –¢–æ–ª—å–∫–æ –û–ß–ï–ù–¨ –¥–ª–∏–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç - –≤–æ–∑–º–æ–∂–Ω–æ –Ω–æ–≤—ã–π —Å–ø–∏–∫–µ—Ä (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 30)
                        current_speaker = (current_speaker + 1) % 2
                        speaker_label = f"Speaker_{chr(65 + current_speaker)}"
                    else:
                        # –ö–æ—Ä–æ—Ç–∫–∏–π —Å–µ–≥–º–µ–Ω—Ç - —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä
                        speaker_label = f"Speaker_{chr(65 + current_speaker)}"
                    
                    segment_path = self._extract_audio_segment(
                        audio, current_pos, silence_start, len(segments)
                    )
                    
                    segments.append({
                        'id': len(segments),
                        'path': segment_path,
                        'start_time': current_pos / 1000.0,
                        'end_time': silence_start / 1000.0,
                        'duration': segment_duration,
                        'speaker': speaker_label,
                        'speaker_confidence': 0.8,  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                        'silence_after': (silence_end - silence_start) / 1000.0
                    })
                    
                    self.logger.debug(f"üé≠ –°–µ–≥–º–µ–Ω—Ç {len(segments)}: {speaker_label}, {segment_duration:.1f}s")
            
            current_pos = silence_end
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–∞—É–∑—ã
        if current_pos < len(audio):
            segment_duration = (len(audio) - current_pos) / 1000.0
            if segment_duration >= min_duration:
                segment_path = self._extract_audio_segment(
                    audio, current_pos, len(audio), len(segments)
                )
                
                # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ —Ç–æ–∂–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–∏–∫–µ—Ä–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ
                if segment_duration > 30:  # –î–ª–∏–Ω–Ω—ã–π –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç - –≤–æ–∑–º–æ–∂–Ω–æ –¥—Ä—É–≥–æ–π —Å–ø–∏–∫–µ—Ä (—É–≤–µ–ª–∏—á–µ–Ω–æ —Å 15)
                    current_speaker = (current_speaker + 1) % 2
                
                segments.append({
                    'id': len(segments),
                    'path': segment_path,
                    'start_time': current_pos / 1000.0,
                    'end_time': len(audio) / 1000.0,
                    'duration': segment_duration,
                    'speaker': f"Speaker_{chr(65 + current_speaker)}",
                    'speaker_confidence': 0.8,
                    'silence_after': 0.0
                })
        
        return segments
    
    def _extract_audio_segment(self, audio: 'AudioSegment', start_ms: int, end_ms: int, segment_id: int) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –∞—É–¥–∏–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —Ñ–∞–π–ª"""
        from pathlib import Path
        
        segment = audio[start_ms:end_ms]
        
        if self.config:
            segment_path = self.config.get_temp_filename(f"speaker_segment_{segment_id}", ".wav")
        else:
            segment_path = f"/tmp/speaker_segment_{segment_id}.wav"
            
        segment.export(str(segment_path), format="wav")
        return str(segment_path)
    
    def _fallback_segmentation(self, audio_path: str) -> List[Dict]:
        """Fallback —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–µ–∑ speaker diarization"""
        from pydub import AudioSegment
        from pydub.silence import split_on_silence
        
        self.logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é –±–µ–∑ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º")
        
        audio = AudioSegment.from_file(audio_path)
        chunks = split_on_silence(
            audio,
            min_silence_len=1000,
            silence_thresh=-40,
            keep_silence=500
        )
        
        segments = []
        current_time = 0
        
        for i, chunk in enumerate(chunks):
            chunk_duration = len(chunk) / 1000.0
            
            if chunk_duration > 1.0:  # –º–∏–Ω–∏–º—É–º 1 —Å–µ–∫—É–Ω–¥–∞
                segment_path = self._extract_audio_segment(
                    audio, int(current_time * 1000), int((current_time + chunk_duration) * 1000), i
                )
                
                segments.append({
                    'id': i,
                    'path': segment_path,
                    'start_time': current_time,
                    'end_time': current_time + chunk_duration,
                    'duration': chunk_duration,
                    'speaker': f"Speaker_{i % 2 + 1}",  # –ü—Ä–æ—Å—Ç–æ–µ —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
                    'speaker_confidence': 0.5,
                    'silence_after': 0.5
                })
            
            current_time += chunk_duration
            
        return segments
    
    def merge_short_segments(self, segments: List[Dict], min_duration: float = 5.0) -> List[Dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–≥–º–µ–Ω—Ç—ã –æ–¥–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        
        Args:
            segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            min_duration: –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∂–µ–ª–∞–µ–º–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç–∞
            
        Returns:
            list: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
        """
        if not segments:
            return segments
            
        merged = []
        current_group = [segments[0]]
        
        for i in range(1, len(segments)):
            current_seg = segments[i]
            prev_seg = segments[i-1]
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –µ—Å–ª–∏ —Ç–æ—Ç –∂–µ —Å–ø–∏–∫–µ—Ä –∏ –æ–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–µ —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫–∞
            if (current_seg['speaker'] == prev_seg['speaker'] and 
                sum(s['duration'] for s in current_group) + current_seg['duration'] < min_duration * 2):
                current_group.append(current_seg)
            else:
                # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
                if len(current_group) > 1:
                    merged_segment = self._merge_segment_group(current_group)
                    merged.append(merged_segment)
                else:
                    merged.append(current_group[0])
                    
                current_group = [current_seg]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥—Ä—É–ø–ø—É
        if current_group:
            if len(current_group) > 1:
                merged_segment = self._merge_segment_group(current_group)
                merged.append(merged_segment)
            else:
                merged.append(current_group[0])
        
        self.logger.info(f"üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {len(segments)} ‚Üí {len(merged)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
        return merged
    
    def _merge_segment_group(self, group: List[Dict]) -> Dict:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≥—Ä—É–ø–ø—É —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≤ –æ–¥–∏–Ω"""
        if not group:
            return {}
            
        first = group[0]
        last = group[-1]
        
        return {
            'id': first['id'],
            'path': first['path'],  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –ø–µ—Ä–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
            'start_time': first['start_time'],
            'end_time': last['end_time'],
            'duration': sum(s['duration'] for s in group),
            'speaker': first['speaker'],
            'speaker_confidence': sum(s['speaker_confidence'] for s in group) / len(group),
            'merged_from': len(group),
            'silence_after': last.get('silence_after', 0.0)
        }
    
    def _detect_gender_for_segments(self, segments: List[Dict]) -> List[Dict]:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–ª –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –≥–æ–ª–æ—Å–∞
        
        Args:
            segments: —Å–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∞—É–¥–∏–æ
            
        Returns:
            segments: —Å–µ–≥–º–µ–Ω—Ç—ã —Å –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–ª–µ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–º –≥–æ–ª–æ—Å–æ–º
        """
        self.logger.info("üé≠ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª–∞ —Å–ø–∏–∫–µ—Ä–æ–≤...")
        
        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≥–æ–ª–æ—Å–æ–≤
        self.used_voices = {'male': 0, 'female': 0}
        speaker_genders = {}  # –ö—ç—à –¥–ª—è —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–ø–∏–∫–µ—Ä–æ–≤
        
        for segment in segments:
            speaker_id = segment['speaker']
            
            # –ï—Å–ª–∏ —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ –ø–æ–ª –¥–ª—è —ç—Ç–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
            if speaker_id in speaker_genders:
                gender = speaker_genders[speaker_id]
            else:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª –ø–æ –∞—É–¥–∏–æ —Å–µ–≥–º–µ–Ω—Ç—É
                gender = self._analyze_voice_gender(segment['path'])
                speaker_genders[speaker_id] = gender
            
            # –ù–∞–∑–Ω–∞—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —ç—Ç–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            voice_id = self._assign_voice_for_speaker(speaker_id, gender)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Å–µ–≥–º–µ–Ω—Ç
            segment['gender'] = gender
            segment['voice_id'] = voice_id
            
            self.logger.debug(f"üé≠ {speaker_id}: {gender}, –≥–æ–ª–æ—Å: {voice_id}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        gender_stats = {}
        for segment in segments:
            gender = segment['gender']
            gender_stats[gender] = gender_stats.get(gender, 0) + 1
        
        self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–æ–≤: {gender_stats}")
        
        return segments
    
    def _analyze_voice_gender(self, audio_path: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –ø–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
        
        Args:
            audio_path: –ø—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É
            
        Returns:
            str: 'male' –∏–ª–∏ 'female'
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ
            y, sr = librosa.load(audio_path, sr=None)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç–æ—Ç—É (F0) - –∫–ª—é—á–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –ø–æ–ª–∞
            pitches, magnitudes = librosa.piptrack(y=y, sr=sr, threshold=0.1)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è F0
            f0_values = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t]
                if pitch > 0:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    f0_values.append(pitch)
            
            if not f0_values:
                # Fallback: –∞–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                return self._analyze_spectral_features(y, sr)
            
            # –ú–µ–¥–∏–∞–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
            median_f0 = np.median(f0_values)
            
            self.logger.debug(f"üéµ F0 –º–µ–¥–∏–∞–Ω–∞: {median_f0:.1f} Hz")
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ
            # –ú—É–∂—á–∏–Ω—ã: –æ–±—ã—á–Ω–æ 85-180 Hz
            # –ñ–µ–Ω—â–∏–Ω—ã: –æ–±—ã—á–Ω–æ 165-265 Hz
            if median_f0 < 150:
                return 'male'
            elif median_f0 > 200:
                return 'female'
            else:
                # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–∞—è –∑–æ–Ω–∞ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                return self._analyze_spectral_features(y, sr)
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–ª–∞: {e}")
            # Fallback: —Å–ª—É—á–∞–π–Ω–æ–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ—Å—Ç–æ–π —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
            return 'male' if len(audio_path) % 2 == 0 else 'female'
    
    def _analyze_spectral_features(self, y: np.ndarray, sr: int) -> str:
        """
        –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–ª–∞
        
        Args:
            y: –∞—É–¥–∏–æ —Å–∏–≥–Ω–∞–ª
            sr: —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
            
        Returns:
            str: 'male' –∏–ª–∏ 'female'
        """
        try:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ (—è—Ä–∫–æ—Å—Ç—å –∑–≤—É–∫–∞)
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
            mean_centroid = np.mean(spectral_centroids)
            
            # –í—ã—á–∏—Å–ª—è–µ–º MFCC (–º–µ–ª-—á–∞—Å—Ç–æ—Ç–Ω—ã–µ –∫–µ–ø—Å—Ç—Ä–∞–ª—å–Ω—ã–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã)
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            mean_mfcc = np.mean(mfccs, axis=1)
            
            self.logger.debug(f"üéµ –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥: {mean_centroid:.1f} Hz")
            
            # –ñ–µ–Ω—Å–∫–∏–µ –≥–æ–ª–æ—Å–∞ –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
            # –∏ –¥—Ä—É–≥–∏–µ MFCC —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            if mean_centroid > 2500:  # –í—ã—Å–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
                return 'female'
            elif mean_centroid < 1500:  # –ù–∏–∑–∫–∏–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
                return 'male'
            else:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º MFCC –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
                # –í—Ç–æ—Ä–æ–π MFCC –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —á–∞—Å—Ç–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä—É–µ—Ç —Å –ø–æ–ª–æ–º
                if len(mean_mfcc) > 1 and mean_mfcc[1] > 0:
                    return 'female'
                else:
                    return 'male'
                    
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return 'male'  # Fallback –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _assign_voice_for_speaker(self, speaker_id: str, gender: str) -> str:
        """
        –ù–∞–∑–Ω–∞—á–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —Å–ø–∏–∫–µ—Ä–∞
        
        Args:
            speaker_id: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–ø–∏–∫–µ—Ä–∞
            gender: –ø–æ–ª —Å–ø–∏–∫–µ—Ä–∞ ('male' –∏–ª–∏ 'female')
            
        Returns:
            str: –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–∞–∑–Ω–∞—á–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞
        """
        if gender not in self.voice_mapping:
            gender = 'male'  # Fallback
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –≥–æ–ª–æ—Å –¥–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª–∞
        available_voices = self.voice_mapping[gender]
        voice_index = self.used_voices[gender] % len(available_voices)
        voice_id = available_voices[voice_index]
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ —Ç–æ–≥–æ –∂–µ –ø–æ–ª–∞
        self.used_voices[gender] += 1
        
        return voice_id